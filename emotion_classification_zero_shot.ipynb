{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ffb9fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/franz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/franz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/franz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# LOADING\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "DIR = '../input/twitter-datasets'\n",
    "\n",
    "\n",
    "def _read_data(path: str) -> List[str]:\n",
    "    with open(path, 'r') as f:\n",
    "        return [x for x in f]\n",
    "\n",
    "\n",
    "def _read_data_with_ids(path: str) -> Tuple[List[str], List[str]]:\n",
    "    index = []\n",
    "    rows = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            id, x = line.split(',', maxsplit=1)\n",
    "            index.append(id)\n",
    "            rows.append(x)\n",
    "    return index, rows\n",
    "\n",
    "\n",
    "def load_train(full=False, dir=DIR, eval_frac=None, x_col='x', y_col='y', neg_label=-1, pos_label=1) -> Union[pd.DataFrame, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    pos_path = os.path.join(\n",
    "        dir, 'train_pos' + ('_full' if full else '') + '.txt')\n",
    "    neg_path = os.path.join(\n",
    "        dir, 'train_neg' + ('_full' if full else '') + '.txt')\n",
    "\n",
    "    pos_rows = _read_data(pos_path)\n",
    "    pos = pd.DataFrame({x_col: pos_rows})\n",
    "    pos[y_col] = pos_label\n",
    "\n",
    "    neg_rows = _read_data(neg_path)\n",
    "    neg = pd.DataFrame({x_col: neg_rows})\n",
    "    neg[y_col] = neg_label\n",
    "\n",
    "    df = pd.concat([pos, neg], ignore_index=True).sample(frac=1).reset_index()\n",
    "    if eval_frac is None:\n",
    "        return df\n",
    "    else:\n",
    "        val = df.sample(frac=eval_frac)\n",
    "        train = df.drop(val.index)\n",
    "        return train, val\n",
    "\n",
    "\n",
    "def load_test(dir=\"../input/cil-text-classification-2022\", x_col='x') -> pd.DataFrame:\n",
    "    path = os.path.join(dir, 'test_data.txt')\n",
    "    index, rows = _read_data_with_ids(path)\n",
    "    df = pd.DataFrame({x_col: rows}, index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# PREPROCESSING\n",
    "\n",
    "from typing import Dict, Optional\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nltk.download('wordnet')\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def to_lower(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains sentences.\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].apply(lambda sentence: sentence.lower())\n",
    "\n",
    "\n",
    "def tokenize(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains sentences.\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].apply(lambda sentence: tokenizer.tokenize(sentence))\n",
    "\n",
    "\n",
    "def remove_tags(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains sentences.\n",
    "    Deprecated in favour of remove_tag_tokens(df: pd.DataFrame)\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].apply(lambda sentence: sentence.replace(\n",
    "        '<user>', '').replace('<url>', '').strip())\n",
    "\n",
    "\n",
    "def remove_tag_tokens(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains tokens.\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].apply(\n",
    "        lambda tokens: [w for w in tokens if not w in ['user', '<url>']])\n",
    "\n",
    "\n",
    "def remove_stopwords(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains tokens.\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].apply(\n",
    "        lambda tokens: [w for w in tokens if not w in stop_words])\n",
    "\n",
    "\n",
    "def lemmatize(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains tokens.\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].apply(\n",
    "        lambda tokens: [lemmatizer.lemmatize(w) for w in tokens])\n",
    "\n",
    "\n",
    "def remove_single_symbols(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains tokens.\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].apply(\n",
    "        lambda tokens: [w for w in tokens if len(w) > 1])\n",
    "\n",
    "\n",
    "def spelling_correction(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains tokens.\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].progress_apply(\n",
    "        lambda tokens: [Word(w).correct() for w in tokens])\n",
    "\n",
    "\n",
    "def replace_user_handles(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains tokens.\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].apply(lambda tokens: [w if not (\n",
    "        w.startswith(\"@\") and len(w) > 1) else \"<user>\" for w in tokens])\n",
    "\n",
    "\n",
    "def replace_urls(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains tokens.\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].apply(lambda tokens: [w if not (w.startswith(\n",
    "        \"http://\") or w.startswith(\"https://\") or w.startswith(\"www.\")) else \"<url>\" for w in tokens])\n",
    "\n",
    "\n",
    "def untokenize(df: pd.DataFrame, x_col='x'):\n",
    "    \"\"\"\n",
    "    To be applied to a dataframe with a column called 'x' that contains tokens.\n",
    "    \"\"\"\n",
    "    df[x_col] = df[x_col].apply(lambda tokens: \" \".join(tokens))\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame, flags: Optional[Dict[str, bool]], x_col='x'):\n",
    "    if flags is not None:\n",
    "        if flags.get('to_lower', False):\n",
    "            to_lower(df, x_col=x_col)\n",
    "        if flags.get('tokenize', False):\n",
    "            tokenize(df, x_col=x_col)\n",
    "        if flags.get('replace_user_handles', False):\n",
    "            replace_user_handles(df, x_col=x_col)\n",
    "        if flags.get('replace_urls', False):\n",
    "            replace_urls(df, x_col=x_col)\n",
    "        if flags.get('remove_tags', False):\n",
    "            remove_tags(df, x_col=x_col)\n",
    "        if flags.get('remove_tag_tokens', False):\n",
    "            remove_tag_tokens(df, x_col=x_col)\n",
    "        if flags.get('remove_stopwords', False):\n",
    "            remove_stopwords(df, x_col=x_col)\n",
    "        if flags.get('lemmatize', False):\n",
    "            lemmatize(df, x_col=x_col)\n",
    "        if flags.get('remove_single_symbols', False):\n",
    "            remove_single_symbols(df, x_col=x_col)\n",
    "        if flags.get('spelling_correction', False):\n",
    "            spelling_correction(df, x_col=x_col)\n",
    "\n",
    "            \n",
    "# SUBMISSION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "from loading import load_test\n",
    "\n",
    "\n",
    "def prepare_model_submission(model: Callable[[pd.DataFrame], np.array], file='submission.csv'):\n",
    "    df = load_test()\n",
    "    y_pred = model(df)\n",
    "    prepare_submission(y_pred, file)\n",
    "\n",
    "\n",
    "def prepare_submission(y_pred: np.ndarray, file='submission.csv'):\n",
    "    df = pd.DataFrame(y_pred, columns=['Prediction'])\n",
    "    df.index += 1\n",
    "    df.to_csv(file, index_label='Id')\n",
    "\n",
    "\n",
    "# BERT INIT\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, ClassLabel\n",
    "from scipy.special import softmax\n",
    "\n",
    "from evaluation import evaluate, evaluate_prob\n",
    "from loading import load_train\n",
    "from preprocessing import preprocess\n",
    "\n",
    "\n",
    "def load(df_train, df_val, preprocessing=None):\n",
    "    # df_train, df_val = load_train(full=full, eval_frac=0.2, x_col='text', y_col='label', neg_label=0, pos_label=1)\n",
    "\n",
    "    preprocess(df_train, flags=preprocessing, x_col='text')\n",
    "    preprocess(df_val, flags=preprocessing, x_col='text')\n",
    "\n",
    "    dataset_train = Dataset.from_pandas(df_train)\n",
    "    dataset_val = Dataset.from_pandas(df_val)\n",
    "\n",
    "    new_features = dataset_train.features.copy()\n",
    "    new_features['label'] = ClassLabel(names=['0', '1'])\n",
    "\n",
    "    dataset_train = dataset_train.cast(new_features)\n",
    "    dataset_val = dataset_val.cast(new_features)\n",
    "\n",
    "    return dataset_train, dataset_val\n",
    "\n",
    "\n",
    "def tokenize(ds, tokenizer, path=None, force=True):\n",
    "    def tokenize_function(ds):\n",
    "        return tokenizer(ds['text'], padding=True, truncation=True)\n",
    "\n",
    "    def load_or_tokenize(ds, path, force):\n",
    "        if not force and path is not None and Path(path).exists():\n",
    "            return Dataset.load_from_disk(path)\n",
    "        else:\n",
    "            ds_tokenized = ds.map(tokenize_function, batched=True)\n",
    "            if path is not None:\n",
    "                ds_tokenized.save_to_disk(path)\n",
    "            return ds_tokenized\n",
    "\n",
    "    return load_or_tokenize(ds, path=path, force=force)\n",
    "\n",
    "\n",
    "def get_BERT(model_name, device):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=4, ignore_mismatched_sizes=True).to(device)\n",
    "    model.save_pretrained(model_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    point_estimates = np.argmax(predictions, axis=1)\n",
    "    point_estimate_eval = evaluate(labels, point_estimates)\n",
    "\n",
    "    prob_estimates = softmax(predictions, axis=1)[:, 1]\n",
    "    prob_estimates_eval = evaluate_prob(labels, prob_estimates)\n",
    "    confidence = np.max(prob_estimates, axis=1)\n",
    "    all_confidence = confidence.mean()\n",
    "    all_confidence_std = confidence.std()\n",
    "    correct_confidence = confidence[labels == point_estimates].mean()\n",
    "    correct_confidence_std = confidence[labels == point_estimates].std()\n",
    "    incorrect_confidence = confidence[labels != point_estimates].mean()\n",
    "    incorrect_confidence_std = confidence[labels != point_estimates].std()\n",
    "\n",
    "    return {\n",
    "        **point_estimate_eval,\n",
    "        **prob_estimates_eval,\n",
    "        'confidence': all_confidence,\n",
    "        'confidence_std': all_confidence_std,\n",
    "        'correct_confidence': correct_confidence,\n",
    "        'correct_confidence_std': correct_confidence_std,\n",
    "        'incorrect_confidence': incorrect_confidence,\n",
    "        'incorrect_confidence_std': incorrect_confidence_std,\n",
    "    }\n",
    "\n",
    "\n",
    "def train(model_name, tokenizer_name, device, df_train, df_val, preprocessing=None, batch_size=32, epochs=1, force_tokenize=True):\n",
    "    dataset_train, dataset_val = load(\n",
    "        df_train, df_val, preprocessing=preprocessing)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    train_tokenized = tokenize(\n",
    "        dataset_train,\n",
    "        tokenizer,\n",
    "        # path=f'bert/cache/train_tokenized__{tokenizer_name}{\"__full\" if full else \"\"}',\n",
    "        force=force_tokenize)\n",
    "    val_tokenized = tokenize(\n",
    "        dataset_val,\n",
    "        tokenizer,\n",
    "        # path=f'bert/cache/val_tokenized__{tokenizer_name}{\"__full\" if full else \"\"}',\n",
    "        force=force_tokenize)\n",
    "\n",
    "    model = get_BERT(model_name, device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='bert_data/test_trainer',\n",
    "        num_train_epochs=epochs,\n",
    "        save_strategy='epoch',\n",
    "        evaluation_strategy='epoch',\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        load_best_model_at_end=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=val_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # val_pred = trainer.predict(val_tokenized)\n",
    "    # y_pred = np.argmax(val_pred.predictions, axis=1)\n",
    "    # y = val_tokenized.to_pandas()['label']\n",
    "    # metrics = evaluate(y, y_pred)\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(args, model_name, tokenizer_name, device, full=False):\n",
    "    print(args)\n",
    "    _, metrics = train(model_name, tokenizer_name, device, full=full, **args)\n",
    "    return -metrics['accuracy']\n",
    "\n",
    "\n",
    "# TRAIN TEST SPLIT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('train_test/train.csv')\n",
    "df_eval = pd.read_csv('train_test/test.csv')\n",
    "\n",
    "\n",
    "def select_train(size=160_000):\n",
    "    return df_train.iloc[:size].drop(['Unnamed: 0'], axis='columns')\n",
    "\n",
    "\n",
    "def select_train_with_cluster(df_cluster_map: pd.DataFrame, cluster: int, size=160_000):\n",
    "    df = pd.merge(df_train, df_cluster_map, on='index')\n",
    "    return df[df['cluster'] == cluster].iloc[:size].drop(['Unnamed: 0'], axis='columns')\n",
    "\n",
    "\n",
    "def select_eval(size=40_000):\n",
    "    return df_eval.iloc[:size].drop(['Unnamed: 0'], axis='columns')\n",
    "\n",
    "\n",
    "def select_eval_with_cluster(df_cluster_map: pd.DataFrame, cluster: int, size=40_000):\n",
    "    df = pd.merge(df_eval, df_cluster_map, on='index')\n",
    "    return df[df['cluster'] == cluster].iloc[:size].drop(['Unnamed: 0'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b17827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "FULL=True\n",
    "\n",
    "task='emotion'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "TOKENIZER = MODEL\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "PREPROCESSING = None\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = get_BERT(model_name, device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER)\n",
    "trainer = Trainer(model, tokenizer=tokenizer)\n",
    "\n",
    "# TEST DATA\n",
    "print(\"load test data\")\n",
    "df_test = load_test(x_col='text')\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "print(\"tokenize test data\")\n",
    "test_tokenized = tokenize(dataset_test, tokenizer, path=f'bert/cache/test_tokenized__{TOKENIZER}')\n",
    "print(\"predict emo labels for test data\")\n",
    "test_pred = trainer.predict(test_tokenized)\n",
    "test_pred = np.argmax(test_pred.predictions, axis=1)\n",
    "print(\"save emo labels for test data\")\n",
    "test_pred = pd.DataFrame(test_pred)\n",
    "test_pred.to_csv(\"test_1_epoch.csv\")\n",
    "\n",
    "# TRAINING DATA\n",
    "print(\"load training data\")\n",
    "df_train = load_train(x_col='text', full=True)\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "print(\"tokenize training data\")\n",
    "train_tokenized = tokenize(dataset_train, tokenizer, path=f'bert/cache/test_tokenized__{TOKENIZER}')\n",
    "print(\"predict emo labels for training data\")\n",
    "train_pred = trainer.predict(train_tokenized)\n",
    "train_pred = np.argmax(train_pred.predictions, axis=1)\n",
    "print(\"save emo labels for training data\")\n",
    "train_pred = pd.DataFrame(train_pred)\n",
    "train_pred.to_csv(\"train_emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8379804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
